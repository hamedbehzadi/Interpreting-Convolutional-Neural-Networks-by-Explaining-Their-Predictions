{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9811a96",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "c9811a96"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from utils.Association import Association\n",
        "import subprocess\n",
        "from torchvision import transforms\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from PIL import Image\n",
        "from utils.mapping import simple_labels\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from itertools import chain, combinations\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e3f4899",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false,
        "id": "0e3f4899",
        "outputId": "697982f8-bd3f-4750-d207-7d343d83c316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['mask_score', 'mask', 'name', 'features', 'perturbed_score', 'true_label', 'image_label'])\n"
          ]
        }
      ],
      "source": [
        "tmp = pickle.load(open(\"data/val_set/1/patches_interpolated_filters.pkl\", \"rb\"))\n",
        "print(tmp[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7b1660",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false,
        "id": "5e7b1660",
        "outputId": "5a166df0-2a0c-47a5-bdb6-c5619ad46823"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:05<00:00, 190.77it/s]\n",
            "100%|██████████| 84616/84616 [00:11<00:00, 7542.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[478, 100, 302, 432, 242, -10000]\n",
            "CPU times: user 16.1 s, sys: 280 ms, total: 16.3 s\n",
            "Wall time: 16.5 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Load all transactions\n",
        "\n",
        "class_ids = range(1000)\n",
        "\n",
        "patches = []\n",
        "for i in tqdm(class_ids):\n",
        "    patches.extend(pickle.load(open(\"data/val_set/\" + str(i) + \"/patches_interpolated_filters.pkl\", \"rb\")))\n",
        "\n",
        "\n",
        "def get_trans(patch):\n",
        "    indices = [(i, x) for i, x in enumerate(patch[\"features\"])]\n",
        "    trans = [i[0] for i in sorted(indices, key=lambda x: x[1], reverse=True)][:5]\n",
        "    trans.append(p[\"true_label\"] * -1 - 10000)\n",
        "    #trans.append(p[\"image_label\"] * -1 - 20000)\n",
        "    return trans\n",
        "\n",
        "\n",
        "transactions = []\n",
        "for p in tqdm(patches):\n",
        "    transactions.append(get_trans(p))\n",
        "\n",
        "print(transactions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea77b4be",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ea77b4be"
      },
      "outputs": [],
      "source": [
        "# author: Bart Goethals, University of Antwerp, Belgium\n",
        "# Adapted by Toon Meynen & Stijn Rosaer\n",
        "\n",
        "\n",
        "def eclat(prefix, minsup, items, start=True):\n",
        "    frequents = []\n",
        "    while items:\n",
        "        i, itids = items.pop()\n",
        "        isupp = len(itids)\n",
        "        if isupp >= minsup:\n",
        "            frequents.append((frozenset(prefix + [i]), isupp))\n",
        "            suffix = []\n",
        "            for j, ojtids in items:\n",
        "                jtids = set(itids) & set(ojtids)\n",
        "                if len(jtids) >= minsup:\n",
        "                    suffix.append((j, jtids))\n",
        "            frequents.extend(\n",
        "                eclat(prefix + [i], minsup, sorted(suffix, key=lambda item: len(item[1]), reverse=True), False))\n",
        "    return frequents\n",
        "\n",
        "\n",
        "def subsets(itemset):\n",
        "    \"\"\" List all strict subsets of an itemset without the empty set or with the empty set if include_empty_set=True\n",
        "        subsets({1,2,3}) --> [{1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}]\n",
        "    \"\"\"\n",
        "    s = list(itemset)\n",
        "    #if len(s) < 3:\n",
        "    if len(s) < 2:\n",
        "        return set()\n",
        "    return map(set, chain.from_iterable(combinations(s, r) for r in range(1, len(s) - 1)))\n",
        "    # return map(set, combinations(s, len(s)-2))\n",
        "\n",
        "\n",
        "def deriveRules(itemsets, minconf):\n",
        "    \"\"\" Returns all rules with conf >= minconf that can be derived from the itemsets.\n",
        "        Return: list of association rules in the format: [(antecedent, consequent, supp, conf), ...]\n",
        "    \"\"\"\n",
        "    search_items = dict(itemsets)\n",
        "    rules = set()\n",
        "    for item_set, supp in itemsets:  #\n",
        "        if len(item_set) > 1:\n",
        "            for subset in subsets(item_set):  # for each subset generate a rule\n",
        "                antecedent = frozenset([i for i in subset if i > 0])\n",
        "                if len(antecedent) < 1:\n",
        "                    continue\n",
        "\n",
        "                consequent = frozenset([i for i in item_set - subset if i < 0])\n",
        "                if len(consequent) == 1 or len(consequent) == 2:\n",
        "                    if len(antecedent) > 1:\n",
        "                        conf = supp / search_items[antecedent]\n",
        "                        if conf >= minconf:\n",
        "                            rules.add(Association(antecedent, consequent, conf, supp))\n",
        "\n",
        "    return rules\n",
        "\n",
        "# counts how often \"items\" occur together, ignores labels in this step.\n",
        "def count(items, tidlist):\n",
        "    tids = [tidlist[i] for i in items if i >= 0]\n",
        "    if tids:\n",
        "        return len(set.intersection(*tids))\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# function that converts a list of transactions to a dictionary going {item -> [trans_1, trans_5, ...]}\n",
        "def tidlist(transactions):\n",
        "    data = {}\n",
        "    trans = 0\n",
        "    for transaction in transactions:\n",
        "        trans += 1\n",
        "        for item in transaction:\n",
        "            if item not in data:\n",
        "                data[item] = set()\n",
        "            data[item].add(trans)\n",
        "    return data\n",
        "\n",
        "def genRules(min_conf, minsup, transactions, all_data):\n",
        "    # generate tidlist of subset\n",
        "    data = tidlist(transactions)\n",
        "    # find frequent sets within this subset\n",
        "    frequent_itemsets = eclat([], minsup, sorted(data.items(), key=lambda item: len(item[1]), reverse=True))\n",
        "    # reweigh these sets over the full dataset\n",
        "    reweighed_sets = []\n",
        "    for i, _ in frequent_itemsets:\n",
        "        # count will count how often this set appears, while disregarding all labels\n",
        "        c = count(i, all_data)\n",
        "        # if set only contains labels this can thus return zero, remove those\n",
        "        if c:\n",
        "            reweighed_sets.append((i, c))\n",
        "    # derive rules from reweighed sets\n",
        "    rules = deriveRules(reweighed_sets, min_conf)\n",
        "    return rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6e662f",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false,
        "id": "2a6e662f",
        "outputId": "f3bdf81b-c1cd-4939-be7d-ef90f2837f2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1001/1001 [00:17<00:00, 58.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9972 rules generated \n",
            "CPU times: user 17.1 s, sys: 27.3 ms, total: 17.2 s\n",
            "Wall time: 17.1 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Generate rules label by label\n",
        "\n",
        "\n",
        "rules = set()\n",
        "all_data = tidlist(transactions)\n",
        "for true_id in tqdm(range(-11000, -9999)):\n",
        "    tmp = genRules(0.5, 2, list(t for t in transactions if true_id in t), all_data)\n",
        "    tmp = sorted(tmp, key=lambda x: x.s * x.c, reverse=True)\n",
        "    rules.update(tmp[:10])\n",
        "rules = list(rules)\n",
        "\n",
        "print(f\"{len(rules)} rules generated \")\n",
        "#rules = genRules(0.8, 20, transactions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a20f8f",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "85a20f8f"
      },
      "outputs": [],
      "source": [
        "# Transactions are no longer needed after generating rules\n",
        "del transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253d0208",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "253d0208",
        "outputId": "7c8bcf32-0a7c-4ae8-9313-9ab7e215f6ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---Removing subset rules---\n",
            "9941 remain after 4.94 seconds\n"
          ]
        }
      ],
      "source": [
        "# Remove rules that are contained within other rules\n",
        "# A -> C & AB -> C\n",
        "\n",
        "\n",
        "print(\"---Removing subset rules---\")\n",
        "t = time.time()\n",
        "new_rules = set()\n",
        "for r in sorted(rules, reverse=True, key=lambda x: len(x.left)):\n",
        "    if not len(new_rules):\n",
        "        new_rules.add(r)\n",
        "\n",
        "    subset = False\n",
        "    for new_rule in new_rules:\n",
        "        if r.right == new_rule.right and r.s == new_rule.s and r.c == new_rule.c:\n",
        "            if all(item in new_rule.left for item in r.left):\n",
        "                subset = True\n",
        "                break\n",
        "\n",
        "    if not subset:\n",
        "        new_rules.add(r)\n",
        "print(f\"{len(new_rules)} remain after {time.time() - t:.2f} seconds\")\n",
        "del rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955de060",
      "metadata": {
        "id": "955de060"
      },
      "outputs": [],
      "source": [
        "### Generate basic association rule based system\n",
        "def generate_filters():\n",
        "    filters = dict()\n",
        "    filters[\"cnn_18\"] = dict()\n",
        "    for i in range(1000):\n",
        "        filters[\"cnn_18\"][i] = set()\n",
        "    for r in new_rules:\n",
        "        # cnn_18 as it only affects final layer\n",
        "        # right side is the label\n",
        "        # left side is the set of filters\n",
        "        filters[\"cnn_18\"][r.right[0] * -1 - 10000].update(r.left)\n",
        "\n",
        "    for i in range(1000):\n",
        "        filters[\"cnn_18\"][i] = list(filters[\"cnn_18\"][i])\n",
        "\n",
        "    with open(\"VEBI/assocL.pickle\", \"wb\") as f:\n",
        "        pickle.dump(filters, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7db0d2dc",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7db0d2dc"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# values to normalize input\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "toTensor = transforms.ToTensor()\n",
        "normalize = transforms.Normalize(mean, std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c378be",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "b9c378be"
      },
      "outputs": [],
      "source": [
        "%load_ext line_profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989cefc4",
      "metadata": {
        "id": "989cefc4"
      },
      "outputs": [],
      "source": [
        "new_rules = sorted(new_rules, key=lambda x: x.c * x.s, reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aebed4b",
      "metadata": {
        "id": "9aebed4b",
        "outputId": "44e02971-b930-48ee-87eb-93e50d46f8f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "84616it [06:35, 214.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5474 groups in 395.02 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "t = time.time()\n",
        "cnn_groups = dict()\n",
        "for index, patch in tqdm(enumerate(patches)):\n",
        "    trans = get_trans(patch)\n",
        "    for rule in sorted(new_rules, key=lambda x: len(x.left), reverse=True):\n",
        "        if all(item in trans for item in rule.left):\n",
        "            if rule.left not in cnn_groups:\n",
        "                cnn_groups[rule.left] = set()\n",
        "            cnn_groups[rule.left].add(index)\n",
        "print(f\"{len(cnn_groups)} groups in {time.time() - t:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49d5884",
      "metadata": {
        "id": "a49d5884",
        "outputId": "e2379775-abc3-4559-864b-5c5f3df07caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1996\n"
          ]
        }
      ],
      "source": [
        "new_groups = dict()\n",
        "for i in cnn_groups:\n",
        "    if len(cnn_groups[i]) >= 20:\n",
        "        new_groups[i] = cnn_groups[i]\n",
        "\n",
        "print(len(new_groups))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093fc741",
      "metadata": {
        "scrolled": true,
        "id": "093fc741",
        "outputId": "1c75fd44-777b-411a-aab2-dd84021fa6bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:45<00:00, 21.76it/s]\n"
          ]
        }
      ],
      "source": [
        "label_dict = dict()\n",
        "for l in tqdm(class_ids):\n",
        "    label_dict[l] = []\n",
        "    for label in cnn_groups:\n",
        "        indices = cnn_groups[label]\n",
        "        group = [patches[i] for i in indices]\n",
        "        scores = dict()\n",
        "        for patch in group:\n",
        "            if patch[\"true_label\"] not in scores:\n",
        "                scores[patch[\"true_label\"]] = 0\n",
        "            scores[patch[\"true_label\"]] += 1\n",
        "        if max(scores, key=scores.get) == l:\n",
        "            label_dict[l].append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ea07b6",
      "metadata": {
        "id": "a3ea07b6"
      },
      "outputs": [],
      "source": [
        "#frozendict.frozendict({'filename': 'ILSVRC2012_val_00001906.JPEG', 'true_label': 8, 'labels': (-1012, -12), 'patch': (44, 100, 99, 155), 'score': 0.8469841})\n",
        "def bar_dict(d, labels=True):\n",
        "    totals = sum([d[line] for line in d])\n",
        "    if labels:\n",
        "        for i in sorted([(simple_labels[line], f\"{d[line] / totals * 100:.1f}%\", d[line]) for line in d],\n",
        "                        key=lambda x: x[2], reverse=True):\n",
        "            if i[2] / totals > 0.01:\n",
        "                print(i[1], \"\\t\", i[0])\n",
        "    else:\n",
        "        for i in sorted([(line, f\"{d[line] / totals * 100:.1f}%\", d[line]) for line in d],\n",
        "                        key=lambda x: x[2], reverse=True):\n",
        "            if i[2] / totals > 0.01:\n",
        "                print(i[1], \"\\t\", i[0])\n",
        "\n",
        "    return {line: d[line] / totals for line in d}\n",
        "\n",
        "\n",
        "class AddAll:\n",
        "    def __init__(self):\n",
        "        self.image = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def add(self, im):\n",
        "        if self.image is None:\n",
        "            self.image = im\n",
        "        else:\n",
        "            self.image += im\n",
        "        self.counter += 1\n",
        "\n",
        "    def normalize(self):\n",
        "        self.image /= self.counter\n",
        "\n",
        "    def display(self, name):\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(self.image)\n",
        "        plt.savefig(f\"plots/{name}.png\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class Score:\n",
        "    def __init__(self):\n",
        "        self.score = 0\n",
        "        self.count = 0\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, original_values, label):\n",
        "        self.buffer.append((original_values, label))\n",
        "\n",
        "\n",
        "    def process(self, distribution):\n",
        "        for ov, _ in self.buffer:\n",
        "            for d in distribution:\n",
        "                if ov[d] > 0 and ov[d] <= 1:\n",
        "                    self.score += ov[d] * distribution[d]\n",
        "            self.count += 1\n",
        "\n",
        "        self.buffer = []\n",
        "\n",
        "    def normalize(self, distribution):\n",
        "        if len(self.buffer):\n",
        "            self.process(distribution)\n",
        "            self.score /= self.count\n",
        "\n",
        "    def display(self):\n",
        "        print(f\"Score: {self.score*100:.3f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d7e7f61",
      "metadata": {
        "id": "8d7e7f61"
      },
      "outputs": [],
      "source": [
        "def display_all_layers():\n",
        "    groups_containing_layer = [(l, cnn_groups[l]) for l in cnn_groups if len(l) == 5]\n",
        "    display_groups(groups_containing_layer, \"len = 5\")\n",
        "\n",
        "def display_layers(layers, DISPLAY=True):\n",
        "    groups_containing_layer = [(l, cnn_groups[l]) for l in cnn_groups if all(item in l for item in layers)]\n",
        "    display_groups(groups_containing_layer, layers, DISPLAY)\n",
        "\n",
        "def display_single_group(totals, all_score, label, indices, layers, DISPLAY):\n",
        "    group = [patches[i] for i in indices]\n",
        "\n",
        "    if DISPLAY:\n",
        "        print(\"=\"*30)\n",
        "        print(\"Groupsize:\", len(group))\n",
        "    group_totals = dict()\n",
        "\n",
        "    if DISPLAY:\n",
        "        width = 16\n",
        "        height = int(np.ceil(min(len(group), 128) / width))\n",
        "        axes = []\n",
        "        plt.rcParams['figure.figsize'] = [14, 14 * (height / width)]\n",
        "        fig = plt.figure()\n",
        "\n",
        "    # for average image\n",
        "    add_all = AddAll()\n",
        "    score = Score()\n",
        "    for index, patch in enumerate(sorted(group, key=lambda x: x[\"mask_score\"], reverse=True)):\n",
        "\n",
        "        if DISPLAY:\n",
        "            # Read file\n",
        "            input = transform(\n",
        "                Image.open(\"data/val_set/\" + str(patch[\"true_label\"]) + \"/img/\" + str(patch[\"name\"])).convert(\n",
        "                    'RGB'))[None, :, :]\n",
        "            p = patch[\"mask\"]\n",
        "\n",
        "            # add to visual output\n",
        "            if index < 128:\n",
        "                axes.append(fig.add_subplot(height, width, index + 1))\n",
        "                plt.imshow(np.transpose(input.data[0].cpu().numpy(), (1, 2, 0))[p[0]:p[1], p[2]:p[3], :])\n",
        "                axes[-1].set_xticks([])\n",
        "                axes[-1].set_yticks([])\n",
        "\n",
        "            # add to averaged image\n",
        "            #add_all.add(np.transpose(input.data[0].cpu().numpy(), (1, 2, 0)))\n",
        "            add_all.add(np.transpose(input.data[0].cpu().numpy(), (1, 2, 0))[p[0]:p[1], p[2]:p[3], :])\n",
        "\n",
        "        # calculate groups score\n",
        "        score.add(patch[\"perturbed_score\"], patch[\"true_label\"])\n",
        "        all_score.add(patch[\"perturbed_score\"], patch[\"true_label\"])\n",
        "\n",
        "        # count labels\n",
        "        for potential_label in [patch[\"true_label\"]]:  # + list(patch[\"labels\"]):\n",
        "            if potential_label < 0:\n",
        "                potential_label *= -1\n",
        "                potential_label -= 10000\n",
        "                if potential_label > 10000:\n",
        "                    potential_label -= 10000\n",
        "\n",
        "            if potential_label not in group_totals:\n",
        "                group_totals[potential_label] = 0\n",
        "                if potential_label not in totals:\n",
        "                    totals[potential_label] = 0\n",
        "            group_totals[potential_label] += 1\n",
        "            totals[potential_label] += 1\n",
        "\n",
        "    if DISPLAY:\n",
        "        fig.suptitle(\"filters: \" + str(label), fontsize=16)\n",
        "\n",
        "        plt.tight_layout(pad=0, h_pad=0, w_pad=0, rect=[0, 0, 1, 0.95])\n",
        "        # plt.subplots_adjust(wspace=0.1, hspace=0.1, left=0, right=1, bottom=0, top=1)\n",
        "        plt.savefig(f\"plots/{str(label)}_full.png\")\n",
        "        plt.show()\n",
        "\n",
        "    if DISPLAY:\n",
        "        add_all.normalize()\n",
        "        add_all.display(str(label))\n",
        "\n",
        "    if DISPLAY:\n",
        "        score.normalize(bar_dict(group_totals))\n",
        "        score.display()\n",
        "\n",
        "    return totals, all_score\n",
        "\n",
        "def display_groups(groups_containing_layer, layers, DISPLAY=True):\n",
        "    print(len(groups_containing_layer))\n",
        "    print(f\"---Displaying {len(groups_containing_layer)} groups containing {str(layers)}---\")\n",
        "\n",
        "    totals = dict()\n",
        "    all_score = Score()\n",
        "    for label, indices in groups_containing_layer:\n",
        "        totals, all_score = display_single_group(totals, all_score, label, indices, layers, DISPLAY)\n",
        "    all_score.normalize(bar_dict(totals))\n",
        "    all_score.display()\n",
        "\n",
        "def display_label(label, DISPLAY=True):\n",
        "    print(f\"--- Displaying {simple_labels[label]} ({label}) ---\")\n",
        "\n",
        "    totals = dict()\n",
        "    all_score = Score()\n",
        "\n",
        "    for layers in label_dict[label]:\n",
        "        indices = cnn_groups[layers]\n",
        "        totals, all_score = display_single_group(totals, all_score, layers, indices, layers, DISPLAY)\n",
        "\n",
        "    all_score.normalize(bar_dict(totals))\n",
        "    all_score.display()\n",
        "\n",
        "    label_importance = defaultdict(int)\n",
        "    for lb in label_dict[label]:\n",
        "        for l in lb:\n",
        "            label_importance[l] += 1\n",
        "\n",
        "    print(f\"--- Filters used for {simple_labels[label]} ---\")\n",
        "    bar_dict(label_importance, False)\n",
        "    return label_importance\n",
        "\n",
        "def display_filter(layer, DISPLAY=True):\n",
        "    display_layers((layer,), DISPLAY)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}